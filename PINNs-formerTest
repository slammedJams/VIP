{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":204744045,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport random\nfrom torch.optim import LBFGS\nfrom tqdm import tqdm\n\nfrom notebookde93c438f9 import *\n\nclass PINNs(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, num_layer):\n        super(PINNs, self).__init__()\n\n        layers = []\n        for i in range(num_layer-1):\n            if i == 0:\n                layers.append(nn.Linear(in_features=in_dim, out_features=hidden_dim))\n                layers.append(nn.Tanh())\n            else:\n                layers.append(nn.Linear(in_features=hidden_dim, out_features=hidden_dim))\n                layers.append(nn.Tanh())\n\n        layers.append(nn.Linear(in_features=hidden_dim, out_features=out_dim))\n\n        self.linear = nn.Sequential(*layers)\n\n    def forward(self, x, t):\n        src = torch.cat((x,t), dim=-1)\n        return self.linear(src)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T23:15:54.751131Z","iopub.execute_input":"2024-11-01T23:15:54.752005Z","iopub.status.idle":"2024-11-01T23:15:54.760571Z","shell.execute_reply.started":"2024-11-01T23:15:54.751967Z","shell.execute_reply":"2024-11-01T23:15:54.759629Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# implementation of PINNsformer\n# paper: PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks\n# link: https://arxiv.org/abs/2307.11833\n# @article{zhao2023pinnsformer,\n#   title={PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks},\n#   author={Zhao, Leo Zhiyuan and Ding, Xueying and Prakash, B Aditya},\n#   journal={arXiv preprint arXiv:2307.11833},\n#   year={2023}\n# }\n\nimport pdb\n\n#from utility_py import get_clones\n\nclass WaveAct(nn.Module):\n    def __init__(self):\n        super(WaveAct, self).__init__() \n        self.w1 = nn.Parameter(torch.ones(1), requires_grad=True)\n        self.w2 = nn.Parameter(torch.ones(1), requires_grad=True)\n\n    def forward(self, x):\n        return self.w1 * torch.sin(x)+ self.w2 * torch.cos(x)\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=256):\n        super(FeedForward, self).__init__() \n        self.linear = nn.Sequential(*[\n            nn.Linear(d_model, d_ff),\n            WaveAct(),\n            nn.Linear(d_ff, d_ff),\n            WaveAct(),\n            nn.Linear(d_ff, d_model)\n        ])\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, heads):\n        super(EncoderLayer, self).__init__()\n\n        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads, batch_first=True)\n        self.ff = FeedForward(d_model)\n        self.act1 = WaveAct()\n        self.act2 = WaveAct()\n        \n    def forward(self, x):\n        x2 = self.act1(x)\n        # pdb.set_trace()\n        x = x + self.attn(x2,x2,x2)[0]\n        x2 = self.act2(x)\n        x = x + self.ff(x2)\n        return x\n\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, heads):\n        super(DecoderLayer, self).__init__()\n\n        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads, batch_first=True)\n        self.ff = FeedForward(d_model)\n        self.act1 = WaveAct()\n        self.act2 = WaveAct()\n\n    def forward(self, x, e_outputs): \n        x2 = self.act1(x)\n        x = x + self.attn(x2, e_outputs, e_outputs)[0]\n        x2 = self.act2(x)\n        x = x + self.ff(x2)\n        return x\n\n\nclass Encoder(nn.Module):\n    def __init__(self, d_model, N, heads):\n        super(Encoder, self).__init__()\n        self.N = N\n        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n        self.act = WaveAct()\n\n    def forward(self, x):\n        for i in range(self.N):\n            x = self.layers[i](x)\n        return self.act(x)\n\nclass Decoder(nn.Module):\n    def __init__(self, d_model, N, heads):\n        super(Decoder, self).__init__()\n        self.N = N\n        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n        self.act = WaveAct()\n        \n    def forward(self, x, e_outputs):\n        for i in range(self.N):\n            x = self.layers[i](x, e_outputs)\n        return self.act(x)\n\n\n\nclass PINNsformer(nn.Module):\n    def __init__(self, d_out, d_model, d_hidden, N, heads):\n        super(PINNsformer, self).__init__()\n\n        self.linear_emb = nn.Linear(2, d_model)\n\n        self.encoder = Encoder(d_model, N, heads)\n        self.decoder = Decoder(d_model, N, heads)\n        self.linear_out = nn.Sequential(*[\n            nn.Linear(d_model, d_hidden),\n            WaveAct(),\n            nn.Linear(d_hidden, d_hidden),\n            WaveAct(),\n            nn.Linear(d_hidden, d_out)\n        ])\n\n    def forward(self, x, t):\n        src = torch.cat((x,t), dim=-1)\n        src = self.linear_emb(src)\n\n        e_outputs = self.encoder(src)\n        d_output = self.decoder(src, e_outputs)\n        output = self.linear_out(d_output)\n        # pdb.set_trace()\n        # raise Exception('stop')\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-11-01T23:15:54.816683Z","iopub.execute_input":"2024-11-01T23:15:54.816984Z","iopub.status.idle":"2024-11-01T23:15:54.838885Z","shell.execute_reply.started":"2024-11-01T23:15:54.816954Z","shell.execute_reply":"2024-11-01T23:15:54.837934Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"a = 0.4  # Thermal diffusivity\nL = 1  # Length of the bar\nn = 1  # Frequency of the sinusoidal initial conditions\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = model.to(device)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T23:15:54.871200Z","iopub.execute_input":"2024-11-01T23:15:54.871558Z","iopub.status.idle":"2024-11-01T23:15:54.875827Z","shell.execute_reply.started":"2024-11-01T23:15:54.871525Z","shell.execute_reply":"2024-11-01T23:15:54.874940Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"seed = 1\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.is_available() \n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)\nstep_size = 1e-4","metadata":{"execution":{"iopub.status.busy":"2024-11-01T23:15:54.880670Z","iopub.execute_input":"2024-11-01T23:15:54.880959Z","iopub.status.idle":"2024-11-01T23:15:54.916825Z","shell.execute_reply.started":"2024-11-01T23:15:54.880928Z","shell.execute_reply":"2024-11-01T23:15:54.915903Z"},"trusted":true},"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"L=1\nres, b_left, b_right, b_upper, b_lower = get_data([0, L], [0, 1], 51, 51)\nres_test, _, _, _, _ = get_data([0, L], [0, 1], 101, 101)\n\n# Define the initial condition for u(x, 0)\ndef u_init(x):\n   \n    return np.sin(n * np.pi * x / L)\nprint(res[:, 0])\n\n# Apply the initial condition to the spatial points at t=0\ninit_cond = u_init(res[:, 0])  # Assuming t=0 corresponds to the first time point\n\n# Extend time sequence\nres = make_time_sequence(res, num_step=5, step=1e-4)\nb_left = make_time_sequence(b_left, num_step=5, step=1e-4)\nb_right = make_time_sequence(b_right, num_step=5, step=1e-4)\nb_upper = make_time_sequence(b_upper, num_step=5, step=1e-4)\nb_lower = make_time_sequence(b_lower, num_step=5, step=1e-4)\n\n# Convert to PyTorch tensors\nres = torch.tensor(res, dtype=torch.float32, requires_grad=True).to(device)\nb_left = torch.tensor(b_left, dtype=torch.float32, requires_grad=True).to(device)\nb_right = torch.tensor(b_right, dtype=torch.float32, requires_grad=True).to(device)\nb_upper = torch.tensor(b_upper, dtype=torch.float32, requires_grad=True).to(device)\nb_lower = torch.tensor(b_lower, dtype=torch.float32, requires_grad=True).to(device)\n\n# Initial condition as tensor\ninit_cond = torch.tensor(init_cond, dtype=torch.float32).to(device)\n\n# Separate spatial (x) and temporal (t) components\nx_res, t_res = res[:, :, 0:1], res[:, :, 1:2]\nx_left, t_left = b_left[:, :, 0:1], b_left[:, :, 1:2]\nx_right, t_right = b_right[:, :, 0:1], b_right[:, :, 1:2]\nx_upper, t_upper = b_upper[:,:,0:1], b_upper[:,:,1:2]\nx_lower, t_lower = b_lower[:,:,0:1], b_lower[:,:,1:2]\n\n\n# Initialize model weights\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T23:15:54.918573Z","iopub.execute_input":"2024-11-01T23:15:54.919192Z","iopub.status.idle":"2024-11-01T23:15:55.122352Z","shell.execute_reply.started":"2024-11-01T23:15:54.919148Z","shell.execute_reply":"2024-11-01T23:15:55.121320Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[0.   0.02 0.04 ... 0.96 0.98 1.  ]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"model = PINNsformer(d_out=1, d_hidden=512, d_model=32, N=1, heads=2).to(device)\nimport torch.optim as optimer\nmodel.apply(init_weights)\noptim = LBFGS(model.parameters(), line_search_fn='strong_wolfe')\n#optim = optimer.Adam(model.parameters(), lr=3e-5)\n\nprint(model)\nprint(get_n_params(model))","metadata":{"execution":{"iopub.status.busy":"2024-11-01T23:15:55.123503Z","iopub.execute_input":"2024-11-01T23:15:55.123813Z","iopub.status.idle":"2024-11-01T23:15:56.097918Z","shell.execute_reply.started":"2024-11-01T23:15:55.123780Z","shell.execute_reply":"2024-11-01T23:15:56.097046Z"},"trusted":true},"outputs":[{"name":"stdout","text":"PINNsformer(\n  (linear_emb): Linear(in_features=2, out_features=32, bias=True)\n  (encoder): Encoder(\n    (layers): ModuleList(\n      (0): EncoderLayer(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n        )\n        (ff): FeedForward(\n          (linear): Sequential(\n            (0): Linear(in_features=32, out_features=256, bias=True)\n            (1): WaveAct()\n            (2): Linear(in_features=256, out_features=256, bias=True)\n            (3): WaveAct()\n            (4): Linear(in_features=256, out_features=32, bias=True)\n          )\n        )\n        (act1): WaveAct()\n        (act2): WaveAct()\n      )\n    )\n    (act): WaveAct()\n  )\n  (decoder): Decoder(\n    (layers): ModuleList(\n      (0): DecoderLayer(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n        )\n        (ff): FeedForward(\n          (linear): Sequential(\n            (0): Linear(in_features=32, out_features=256, bias=True)\n            (1): WaveAct()\n            (2): Linear(in_features=256, out_features=256, bias=True)\n            (3): WaveAct()\n            (4): Linear(in_features=256, out_features=32, bias=True)\n          )\n        )\n        (act1): WaveAct()\n        (act2): WaveAct()\n      )\n    )\n    (act): WaveAct()\n  )\n  (linear_out): Sequential(\n    (0): Linear(in_features=32, out_features=512, bias=True)\n    (1): WaveAct()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): WaveAct()\n    (4): Linear(in_features=512, out_features=1, bias=True)\n  )\n)\n453561\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"#print(x_res)\ndef check_nan(tensor, name):\n    if torch.isnan(tensor).any():\n        print(f\"NaNs detected in {name}\")\nloss_track = []\na = 0.4  # Thermal diffusivity\nL = 1  # Length of the bar\nn = 1  # Frequency of the sinusoidal initial conditions\nk= 0 \nt_ic = torch.zeros_like(x_lower)\nfor i in tqdm(range(250)):\n    k+=1\n    if k == 100:\n            print(\"progress\")\n            k=0\n    def closure():\n        \n        pred_res = model(x_res, t_res)\n        pred_left = model(x_left, t_left) #  Left boundary of the spatial domain (x=0) over all times.\n        pred_right = model(x_right, t_right) # t_right is 1\n        #print(pred_res)\n        pred_ic = model(x_right,t_ic )\n        pred_upper = model(x_upper, t_upper) #  All spatial points at the final time step (x=1).\n        pred_lower = model(x_lower, t_lower) #  All spatial points at the initial time step (x=0).\n        pred_res_t0 = pred_res[:, 0, :]\n\n\n        u_x = torch.autograd.grad(pred_res, x_res, grad_outputs=torch.ones_like(pred_res), retain_graph=True, create_graph=True)[0]\n        u_xx = torch.autograd.grad(u_x, x_res, grad_outputs=torch.ones_like(u_x), retain_graph=True, create_graph=True)[0]\n        u_t = torch.autograd.grad(pred_res, t_res, grad_outputs=torch.ones_like(pred_res), retain_graph=True, create_graph=True)[0]\n\n        \n        check_nan(u_x, 'u_x')\n        check_nan(u_xx, 'u_xx')\n        check_nan(u_t, 'u_t')\n       \n        alpha =a   # thermal diffusivity (can be modified)\n        #1. PDE LOSS eesidual\n        loss_res =torch.mean((alpha * u_xx - u_t )**2 )\n\n\n        #2. Loss for initial condition\n\n        loss_ic = torch.mean((pred_ic- torch.sin(n * torch.pi * x_right / L))**2)\n        \n        \n        # 3. Boundary conditions: enforce Dirichlet BCs (u(0,t) = u(L,t) = 0)\n        loss_bc_1 =loss_bc = torch.mean((pred_upper) ** 2) + torch.mean((pred_lower) ** 2)\n        #loss_bc_2 =  torch.mean((pred_left-pred_right) ** 2) \n        loss_bc = loss_bc_1\n        \n        loss_track.append([loss_res.item(), loss_ic.item(), loss_bc.item()])\n\n        loss = 2*loss_res + loss_ic + loss_bc\n        optim.zero_grad()\n        loss.backward()\n        return loss\n\n    optim.step(closure)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T23:15:56.100092Z","iopub.execute_input":"2024-11-01T23:15:56.100540Z","iopub.status.idle":"2024-11-01T23:16:46.001620Z","shell.execute_reply.started":"2024-11-01T23:15:56.100503Z","shell.execute_reply":"2024-11-01T23:16:46.000104Z"},"trusted":true},"outputs":[{"name":"stderr","text":"  0%|          | 0/250 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n  5%|▍         | 12/250 [00:49<16:14,  4.10s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 59\u001b[0m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/lbfgs.py:437\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[0;32m--> 437\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m    441\u001b[0m opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/lbfgs.py:45\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     43\u001b[0m g \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# evaluate objective and gradient using initial step\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m f_new, g_new \u001b[38;5;241m=\u001b[39m \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     47\u001b[0m gtd_new \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mdot(d)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/lbfgs.py:435\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/lbfgs.py:289\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m--> 289\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    290\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[12], line 56\u001b[0m, in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mloss_res \u001b[38;5;241m+\u001b[39m loss_ic \u001b[38;5;241m+\u001b[39m loss_bc\n\u001b[1;32m     55\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 56\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"print(f\"Length of loss_track: {len(loss_track)}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T23:16:46.002792Z","iopub.status.idle":"2024-11-01T23:16:46.003299Z","shell.execute_reply.started":"2024-11-01T23:16:46.003026Z","shell.execute_reply":"2024-11-01T23:16:46.003051Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Loss Res: {:4f}, Loss_IC: {:4f}, loss_bc: {:4f}'.format(loss_track[-1][0], loss_track[-1][1], loss_track[-1][2]))\nprint('Train Loss: {:4f}'.format(np.sum(loss_track[-1])))\n\ntorch.save(model.state_dict(), './1dreaction_pinnsformer.pt')","metadata":{"execution":{"iopub.status.busy":"2024-11-01T23:16:46.004715Z","iopub.status.idle":"2024-11-01T23:16:46.005076Z","shell.execute_reply.started":"2024-11-01T23:16:46.004900Z","shell.execute_reply":"2024-11-01T23:16:46.004918Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Convert loss_track to a NumPy array for easier slicing\nloss_track = np.array(loss_track)\n\n# Extract each loss component\nresidual_loss = loss_track[:, 0]\nboundary_condition_loss = loss_track[:, 2]\ninitial_condition_loss = loss_track[:, 1]\n\n# Plot loss components\nplt.figure(figsize=(12, 6))\nplt.plot(residual_loss, label='Residual Loss', color='r')\nplt.plot(boundary_condition_loss, label='Boundary Condition Loss', color='g')\nplt.plot(initial_condition_loss, label='Initial Condition Loss', color='b')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss Components During Optimization')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T23:16:46.006329Z","iopub.status.idle":"2024-11-01T23:16:46.006694Z","shell.execute_reply.started":"2024-11-01T23:16:46.006511Z","shell.execute_reply":"2024-11-01T23:16:46.006529Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#Visualization for 1D heat equation\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#Generate the test data with the correct dimensions for the heat equation\nres_test = make_time_sequence(res_test, num_step=5, step=1e-4) \nres_test = torch.tensor(res_test, dtype=torch.float32, requires_grad=True).to(device)\nx_test, t_test = res_test[:,:,0:1], res_test[:,:,1:2]\n\n# Predict using the model\nwith torch.no_grad():\n    pred = model(x_test, t_test)[:, 0:1]\n    pred = pred.cpu().detach().numpy()\n\n# Reshape the prediction to fit 101x101 grid (space and time)\npred = pred.reshape(101, 101)\n\n#Final Desired Equation\ndef u_ana(x, t, alpha=0.4, L=1, n=1):\n    return np.exp(-(n**2 * np.pi**2 * alpha * t) / (L**2)) * np.sin(n * np.pi * x / L)\n    #return np.exp(-n**2 * np.pi**2 * alpha * t) * np.sin(n * np.pi * x / L)\n\n#Get the test data again for analytical solution comparison\nres_test, _, _, _, _ = get_data([0, L], [0, 1], 101, 101)\n\n# Compute the analytical solution for the heat equation\nu = u_ana(res_test[:,0], res_test[:,1]).reshape(101, 101)\n\n#Compute relative errors (L1 and L2)\nrl1 = np.sum(np.abs(u - pred)) / np.sum(np.abs(u))\nrl2 = np.sqrt(np.sum((u - pred) ** 2) / np.sum(u ** 2))\n\nprint('Relative L1 error: {:4f}'.format(rl1))\nprint('Relative L2 error: {:4f}'.format(rl2))\n\n#Visualization of predicted solution u(x,t)\nplt.figure(figsize=(4, 3))\nplt.imshow(pred, extent=[0, L, 1, 0], aspect='auto')\nplt.xlabel('x')\nplt.ylabel('t')\nplt.title('Predicted u(x,t) - 1D Heat Equation')\nplt.colorbar()\nplt.tight_layout()\nplt.savefig('./1dheat_pinnsformer_pred.png')\nplt.show()\n\n#Visualization of analytical solution u_ana(x,t)\nplt.figure(figsize=(4, 3))\nplt.imshow(u, extent=[0, L, 1, 0], aspect='auto')\nplt.xlabel('x')\nplt.ylabel('t')\nplt.title('Analytical u(x,t) - 1D Heat Equation')\nplt.colorbar()\nplt.tight_layout()\nplt.savefig('./1dheat_analytical.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T23:16:46.008349Z","iopub.status.idle":"2024-11-01T23:16:46.008856Z","shell.execute_reply.started":"2024-11-01T23:16:46.008575Z","shell.execute_reply":"2024-11-01T23:16:46.008599Z"},"trusted":true},"outputs":[],"execution_count":null}]}